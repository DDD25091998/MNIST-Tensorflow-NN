# -*- coding: utf-8 -*-
"""Neural Network with Tensorflow

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kAoSyFjOBCKFdg4INnupX9O69CyI1MQ5

#More complex project with Tensorflow MNIST

**Run on Google Collab**

I created a more complex project to classify MNIST numbers to show I understand more complicated notions.
"""

!conda install tensorflow
import keras
!conda install keras

#Loading packages 

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import SGD
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.losses import categorical_crossentropy
from keras import metrics
from sklearn.metrics import accuracy_score

# Loading the MNIST dataset
(train_images, train_targets), (test_images, test_targets) = mnist.load_data()

# We reshape the images in our sample in a 28 by 28 grid
X_train = train_images.reshape((60000, 28 * 28))
X_test = test_images.reshape((10000, 28 * 28))


X_train = X_train.astype('float32') / 255
X_test = X_test.astype('float32') / 255

# One-hot encoding of expected results
y_train = to_categorical(train_targets).astype('int')
y_test = to_categorical(test_targets).astype('int')

# Creating a (784, 512, 256, 10) model
model = Sequential()
# This is the first layer?
model.add(Dense(512,activation='relu', input_shape=(28 * 28,)))
model.add(Dense(256,activation='relu' ))
model.add(Dropout(0.1))
model.add(Dense(10, activation='softmax')) 


# Describe the model
model.summary()

# Compiling the model is essential before running it
model.compile(loss = categorical_crossentropy, 
              optimizer = 'adam',
              metrics = [metrics.categorical_accuracy])
#Training the model
history = model.fit(X_train,
                    y_train,
                    validation_split = 0.1,
                    epochs = 20, #I don't run too many epochs to not take too much computational power
                    batch_size = 512)

# Plot Loss
plt.plot(history.history['categorical_accuracy'])
plt.title('model training accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.show()

#Check accuracy at latest epoch
y_prediction = model.predict(X_test)
predictions = np.argmax(y_prediction, axis=1)
y_prediction = to_categorical(predictions)
print('Accuracy = %.3f' % accuracy_score(y_prediction, y_test))
